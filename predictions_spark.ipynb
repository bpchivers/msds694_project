{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "## setup\n",
    "conf = SparkConf().setAppName(\"final_project\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def toFloatSafe(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except ValueError:\n",
    "        return str(v) #if it is not a float type return as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and convert data\n",
    "filename = \"./../data/311_Cases_small.csv\"\n",
    "\n",
    "data_raw = sc.textFile(filename)\\\n",
    "             .map(lambda x: x.split(\",\"))\n",
    "\n",
    "data_raw = data_raw.map(lambda row:  [toFloatSafe(x) for x in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Create Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"closing_time\", FloatType(),False),\n",
    "     StructField(\"neighborhood\", StringType(),False),\n",
    "     StructField(\"category\", StringType(),False),\n",
    "     StructField(\"police_district\", StringType(),False),\n",
    "    StructField(\"responsible_agency\", StringType(), False),\n",
    "    StructField(\"source\", StringType(), False)\n",
    "])\n",
    "\n",
    "df = ss.createDataFrame(data_raw.map(lambda x : Row(x[0],x[1],x[2],x[3],x[4], x[5])), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('closing_time', 'float'),\n",
       " ('neighborhood', 'string'),\n",
       " ('category', 'string'),\n",
       " ('police_district', 'string'),\n",
       " ('responsible_agency', 'string'),\n",
       " ('source', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.show()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Numericalize Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String to Numbers\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def indexStringColumns(df, cols):\n",
    "    #variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnumeric = indexStringColumns(df, ['closing_time', \"neighborhood\", \"category\", \"police_district\", \"responsible_agency\", \"source\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot Encoding\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "dfhot = oneHotEncodeColumns(dfnumeric, [\"neighborhood\", \"category\", \"police_district\", \"responsible_agency\", \"source\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+---------------+---------------+------------------+-------------+\n",
      "|closing_time|    neighborhood|       category|police_district|responsible_agency|       source|\n",
      "+------------+----------------+---------------+---------------+------------------+-------------+\n",
      "|         3.0|(117,[36],[1.0])| (42,[0],[1.0])| (10,[1],[1.0])|   (208,[0],[1.0])|(8,[0],[1.0])|\n",
      "|        30.0|(117,[23],[1.0])| (42,[1],[1.0])| (10,[3],[1.0])|   (208,[0],[1.0])|(8,[0],[1.0])|\n",
      "|         3.0| (117,[6],[1.0])| (42,[9],[1.0])| (10,[3],[1.0])|   (208,[2],[1.0])|(8,[0],[1.0])|\n",
      "|         2.0|(117,[36],[1.0])| (42,[1],[1.0])| (10,[1],[1.0])|   (208,[6],[1.0])|(8,[0],[1.0])|\n",
      "|         3.0|(117,[10],[1.0])| (42,[1],[1.0])| (10,[8],[1.0])|   (208,[0],[1.0])|(8,[0],[1.0])|\n",
      "|        24.0|(117,[35],[1.0])|(42,[14],[1.0])| (10,[0],[1.0])|  (208,[52],[1.0])|(8,[0],[1.0])|\n",
      "|         0.0| (117,[0],[1.0])| (42,[1],[1.0])| (10,[0],[1.0])|   (208,[0],[1.0])|(8,[3],[1.0])|\n",
      "|         0.0| (117,[2],[1.0])| (42,[0],[1.0])| (10,[0],[1.0])|   (208,[0],[1.0])|(8,[0],[1.0])|\n",
      "|         2.0|(117,[12],[1.0])| (42,[0],[1.0])| (10,[2],[1.0])|   (208,[0],[1.0])|(8,[0],[1.0])|\n",
      "|        11.0|(117,[15],[1.0])|(42,[10],[1.0])| (10,[4],[1.0])|   (208,[0],[1.0])|(8,[0],[1.0])|\n",
      "|      1089.0|(117,[87],[1.0])|(42,[12],[1.0])| (10,[3],[1.0])|  (208,[13],[1.0])|(8,[0],[1.0])|\n",
      "|         1.0|(117,[22],[1.0])| (42,[1],[1.0])| (10,[2],[1.0])|   (208,[0],[1.0])|(8,[1],[1.0])|\n",
      "|         0.0|(117,[22],[1.0])| (42,[0],[1.0])| (10,[2],[1.0])|   (208,[1],[1.0])|(8,[0],[1.0])|\n",
      "|         0.0| (117,[4],[1.0])| (42,[7],[1.0])| (10,[2],[1.0])|  (208,[15],[1.0])|(8,[0],[1.0])|\n",
      "|         1.0| (117,[9],[1.0])| (42,[2],[1.0])| (10,[6],[1.0])|   (208,[0],[1.0])|(8,[0],[1.0])|\n",
      "|        41.0| (117,[0],[1.0])| (42,[0],[1.0])| (10,[0],[1.0])|  (208,[25],[1.0])|(8,[0],[1.0])|\n",
      "|         0.0|(117,[39],[1.0])| (42,[0],[1.0])| (10,[0],[1.0])|   (208,[1],[1.0])|(8,[3],[1.0])|\n",
      "|         5.0|(117,[20],[1.0])| (42,[1],[1.0])| (10,[1],[1.0])|   (208,[0],[1.0])|(8,[0],[1.0])|\n",
      "|         0.0| (117,[3],[1.0])| (42,[0],[1.0])| (10,[0],[1.0])|   (208,[1],[1.0])|(8,[0],[1.0])|\n",
      "|        23.0|(117,[18],[1.0])| (42,[5],[1.0])| (10,[0],[1.0])|   (208,[2],[1.0])|(8,[0],[1.0])|\n",
      "+------------+----------------+---------------+---------------+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfhot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Create a Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "input_cols= [\"neighborhood\", \"category\", \"police_district\", \"responsible_agency\", \"source\"]\n",
    "\n",
    "#VectorAssembler takes a number of collumn names(inputCols) and output column name (outputCol)\n",
    "#and transforms a DataFrame to assemble the values in inputCols into one single vector with outputCol.\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=input_cols)\n",
    "#lpoints - labeled data.\n",
    "lpoints = va.transform(dfhot).select(\"features\", \"closing_time\").withColumnRenamed(\"closing_time\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| label|\n",
      "+--------------------+------+\n",
      "|(385,[36,117,160,...|   3.0|\n",
      "|(385,[23,118,162,...|  30.0|\n",
      "|(385,[6,126,162,1...|   3.0|\n",
      "|(385,[36,118,160,...|   2.0|\n",
      "|(385,[10,118,167,...|   3.0|\n",
      "|(385,[35,131,159,...|  24.0|\n",
      "|(385,[0,118,159,1...|   0.0|\n",
      "|(385,[2,117,159,1...|   0.0|\n",
      "|(385,[12,117,161,...|   2.0|\n",
      "|(385,[15,127,163,...|  11.0|\n",
      "|(385,[87,129,162,...|1089.0|\n",
      "|(385,[22,118,161,...|   1.0|\n",
      "|(385,[22,117,161,...|   0.0|\n",
      "|(385,[4,124,161,1...|   0.0|\n",
      "|(385,[9,119,165,1...|   1.0|\n",
      "|(385,[0,117,159,1...|  41.0|\n",
      "|(385,[39,117,159,...|   0.0|\n",
      "|(385,[20,118,160,...|   5.0|\n",
      "|(385,[3,117,159,1...|   0.0|\n",
      "|(385,[18,122,159,...|  23.0|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lpoints.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* TEMP ******************* # (currently under construction, waiting for data)\n",
    "# (later, divide the dataset by time, and have two files reading in,\n",
    "#  instead of a train test split)\n",
    "\n",
    "# Train test split\n",
    "\n",
    "splits = lpoints.randomSplit([0.8, 0.2])\n",
    "\n",
    "adulttrain = splits[0].cache()\n",
    "adultvalid = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(regParam=0.01, maxIter=10, fitIntercept=True)\n",
    "lrmodel = lr.fit(adulttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.3843\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "predictions = lrmodel.transform(adultvalid)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "print(\"F1 = {:.4f}\".format(f1)) # F1 = 0.3843"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
